{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1L0Amxga2WV"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoJFTUBEaqQ3"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import time\n",
        "import torch.utils.model_zoo as model_zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRvf2fDqsiza"
      },
      "source": [
        "# Setting Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt-F4C1xsi-d"
      },
      "source": [
        "# setting gpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj4VSazaIdVX"
      },
      "source": [
        "# Anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw_xlM9XIfKU"
      },
      "source": [
        "class Anchors(nn.Module):\n",
        "  def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n",
        "    super(Anchors, self).__init__()\n",
        "\n",
        "    if pyramid_levels is None:\n",
        "      self.pyramid_levels = [3, 4, 5, 6, 7]\n",
        "    else:\n",
        "      self.pyramid_levels = pyramid_levels\n",
        "\n",
        "    if strides is None:\n",
        "      self.strides = [2 ** x for x in self.pyramid_levels]\n",
        "    if sizes is None:\n",
        "      self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
        "    if ratios is None:\n",
        "      self.ratios = np.array([0.5, 1, 2])\n",
        "    if scales is None:\n",
        "      self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
        "\n",
        "  def forward(self, image):\n",
        "        \n",
        "    image_shape = image.shape[2:]\n",
        "    image_shape = np.array(image_shape)\n",
        "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
        "\n",
        "    # compute anchors over all pyramid levels\n",
        "    all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
        "\n",
        "    for idx, p in enumerate(self.pyramid_levels):\n",
        "      anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
        "      shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
        "      all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
        "\n",
        "    all_anchors = np.expand_dims(all_anchors, axis=0)\n",
        "\n",
        "    return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n",
        "  \n",
        "  def generate_anchors(base_size=16, ratios=None, scales=None):\n",
        "    \"\"\"\n",
        "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
        "    scales w.r.t. a reference window.\n",
        "    \"\"\"\n",
        "\n",
        "    if ratios is None:\n",
        "      ratios = np.array([0.5, 1, 2])\n",
        "\n",
        "    if scales is None:\n",
        "      scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
        "\n",
        "    num_anchors = len(ratios) * len(scales)\n",
        "\n",
        "    # initialize output anchors\n",
        "    anchors = np.zeros((num_anchors, 4))\n",
        "\n",
        "    # scale base_size\n",
        "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
        "\n",
        "    # compute areas of anchors\n",
        "    areas = anchors[:, 2] * anchors[:, 3]\n",
        "\n",
        "    # correct for ratios\n",
        "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
        "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
        "\n",
        "    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
        "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
        "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
        "\n",
        "    return anchors\n",
        "  \n",
        "  def compute_shape(image_shape, pyramid_levels):\n",
        "    \"\"\"Compute shapes based on pyramid levels.\n",
        "    :param image_shape:\n",
        "    :param pyramid_levels:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    image_shape = np.array(image_shape[:2])\n",
        "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
        "    return image_shape\n",
        "\n",
        "  def anchors_for_shape(\n",
        "    image_shape,\n",
        "    pyramid_levels=None,\n",
        "    ratios=None,\n",
        "    scales=None,\n",
        "    strides=None,\n",
        "    sizes=None,\n",
        "    shapes_callback=None,\n",
        "    ):\n",
        "\n",
        "    image_shapes = compute_shape(image_shape, pyramid_levels)\n",
        "\n",
        "    # compute anchors over all pyramid levels\n",
        "    all_anchors = np.zeros((0, 4))\n",
        "    for idx, p in enumerate(pyramid_levels):\n",
        "      anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n",
        "      shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n",
        "      all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
        "\n",
        "    return all_anchors\n",
        "\n",
        "  def shift(shape, stride, anchors):\n",
        "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
        "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
        "\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "\n",
        "    shifts = np.vstack((\n",
        "        shift_x.ravel(), shift_y.ravel(),\n",
        "        shift_x.ravel(), shift_y.ravel()\n",
        "    )).transpose()\n",
        "\n",
        "    # add A anchors (1, A, 4) to\n",
        "    # cell K shifts (K, 1, 4) to get\n",
        "    # shift anchors (K, A, 4)\n",
        "    # reshape to (K*A, 4) shifted anchors\n",
        "    A = anchors.shape[0]\n",
        "    K = shifts.shape[0]\n",
        "    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
        "    all_anchors = all_anchors.reshape((K * A, 4))\n",
        "\n",
        "    return all_anchors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG5KlFggJf8y"
      },
      "source": [
        "# Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2a1qDhKJiHO"
      },
      "source": [
        "class BBoxTransform(nn.Module):\n",
        "\n",
        "  def __init__(self, mean=None, std=None):\n",
        "    super(BBoxTransform, self).__init__()\n",
        "    if mean is None:\n",
        "      self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32)).cuda()\n",
        "    else:\n",
        "      self.mean = mean\n",
        "    if std is None:\n",
        "      self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32)).cuda()\n",
        "    else:\n",
        "      self.std = std\n",
        "  \n",
        "  def forward(self, boxes, deltas):\n",
        "\n",
        "    widths  = boxes[:, :, 2] - boxes[:, :, 0]\n",
        "    heights = boxes[:, :, 3] - boxes[:, :, 1]\n",
        "    ctr_x   = boxes[:, :, 0] + 0.5 * widths\n",
        "    ctr_y   = boxes[:, :, 1] + 0.5 * heights\n",
        "\n",
        "    dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n",
        "    dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n",
        "    dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n",
        "    dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n",
        "\n",
        "    pred_ctr_x = ctr_x + dx * widths\n",
        "    pred_ctr_y = ctr_y + dy * heights\n",
        "    pred_w     = torch.exp(dw) * widths\n",
        "    pred_h     = torch.exp(dh) * heights\n",
        "\n",
        "    pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n",
        "    pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n",
        "    pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n",
        "    pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n",
        "\n",
        "    pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n",
        "\n",
        "    return pred_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcxhjsivJ1Bg"
      },
      "source": [
        "class ClipBoxes(nn.Module):\n",
        "\n",
        "  def __init__(self, width=None, height=None):\n",
        "    super(ClipBoxes, self).__init__()\n",
        "\n",
        "  def forward(self, boxes, img):\n",
        "\n",
        "    batch_size, num_channels, height, width = img.shape\n",
        "\n",
        "    boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n",
        "    boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n",
        "\n",
        "    boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n",
        "    boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n",
        "      \n",
        "    return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiGgqnq_YTE7"
      },
      "source": [
        "# Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m86ovbtKYSI1"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      residual = self.downsample(x)\n",
        "\n",
        "    out += residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcQ917b-y7YS"
      },
      "source": [
        "# Feature Pyramid Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucSjdKUfy7l-"
      },
      "source": [
        "class PyramidFeatures(nn.Module):\n",
        "  def __init__(self, C2_size, C3_size, C4_size, C5_size, feature_size=256, use_l2_features=True):\n",
        "    super(PyramidFeatures, self).__init__()\n",
        "    self.use_l2_features = use_l2_features\n",
        "        \n",
        "    # upsample C5 to get P5 from the FPN paper\n",
        "    self.P5_1           = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P5_upsampled   = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    self.P5_2           = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # add P5 elementwise to C4\n",
        "    self.P4_1           = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P4_upsampled   = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    self.P4_2           = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # add P4 elementwise to C3\n",
        "    self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # add P3 elementwise to C2\n",
        "    self.P2_1 = nn.Conv2d(C2_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P2_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
        "    self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
        "    self.P7_1 = nn.ReLU()\n",
        "    self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "\n",
        "    C2, C3, C4, C5 = inputs\n",
        "\n",
        "    P5_x = self.P5_1(C5)\n",
        "    P5_upsampled_x = self.P5_upsampled(P5_x)\n",
        "    P5_x = self.P5_2(P5_x)\n",
        "    \n",
        "    P4_x = self.P4_1(C4)\n",
        "    P4_x = P5_upsampled_x + P4_x\n",
        "    P4_upsampled_x = self.P4_upsampled(P4_x)\n",
        "    P4_x = self.P4_2(P4_x)\n",
        "\n",
        "    P3_x = self.P3_1(C3)\n",
        "    P3_x = P3_x + P4_upsampled_x\n",
        "    P3_upsampled_x = self.P3_upsampled(P3_x)\n",
        "    P3_x = self.P3_2(P3_x)\n",
        "\n",
        "    if self.use_l2_features:\n",
        "      P2_x = self.P2_1(C2)\n",
        "      P2_x = P2_x + P3_upsampled_x\n",
        "      P2_x = self.P2_2(P2_x)\n",
        "\n",
        "      P6_x = self.P6(C5)\n",
        "\n",
        "      P7_x = self.P7_1(P6_x)\n",
        "      P7_x = self.P7_2(P7_x)\n",
        "\n",
        "      if self.use_l2_features:\n",
        "        return [P2_x, P3_x, P4_x, P5_x, P6_x, P7_x]\n",
        "      else:\n",
        "        return [P3_x, P4_x, P5_x, P6_x, P7_x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVAFtm-YltGH"
      },
      "source": [
        "# Box Regression Subnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsfPxxk9lyCh"
      },
      "source": [
        "class RegressionModel(nn.Module):\n",
        "  \n",
        "  # creates the default regression submodel,\n",
        "  # it predicts regression values for each anchor.\n",
        "  \n",
        "  def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n",
        "    super(RegressionModel, self).__init__()\n",
        "        \n",
        "    self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act3 = nn.ReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act4 = nn.ReLU()\n",
        "\n",
        "    self.output = nn.Conv2d(feature_size, num_anchors*4, kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.act1(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.act2(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.act3(out)\n",
        "\n",
        "    out = self.conv4(out)\n",
        "    out = self.act4(out)\n",
        "\n",
        "    out = self.output(out)\n",
        "\n",
        "    # out is B x C x W x H, with C = 4*num_anchors\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "\n",
        "    return out.contiguous().view(out.shape[0], -1, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOYAab_Dlzcy"
      },
      "source": [
        "# Classification Subnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUqD-bwfl3jV"
      },
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "\n",
        "  # creates the classification submodel, \n",
        "  # it predicts classes for each anchor.\n",
        "\n",
        "  def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256, dropout=0.5):\n",
        "    super(ClassificationModel, self).__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.num_anchors = num_anchors\n",
        "        \n",
        "    self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act3 = nn.ReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act4 = nn.ReLU()\n",
        "\n",
        "    self.output = nn.Conv2d(feature_size, num_anchors*num_classes, kernel_size=3, padding=1)\n",
        "    self.output_act = nn.Sigmoid()\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.act1(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.act2(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.act3(out)\n",
        "\n",
        "    out = self.conv4(out)\n",
        "    out = self.act4(out)\n",
        "\n",
        "    if self.dropout > 0:\n",
        "      out = F.dropout(out, self.dropout, self.training)\n",
        "\n",
        "    out = self.output(out)\n",
        "    out = self.output_act(out)\n",
        "\n",
        "    # out is B x C x W x H, with C = n_classes + n_anchors\n",
        "    out1 = out.permute(0, 2, 3, 1)\n",
        "\n",
        "    batch_size, width, height, channels = out1.shape\n",
        "\n",
        "    out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n",
        "\n",
        "    return out2.contiguous().view(x.shape[0], -1, self.num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkukUbRRl_tm"
      },
      "source": [
        "# Global Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrWfKgkBmBvF"
      },
      "source": [
        "class GlobalClassificationModel(nn.Module):\n",
        "  def __init__(self, num_features_in, num_classes=80, feature_size=256, dropout=0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, dilation=1, padding=0)\n",
        "    self.fc = nn.Linear(feature_size*2, num_classes)\n",
        "    self.output_act = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(x, 2)\n",
        "    out = self.conv1(out)\n",
        "    out = F.relu(out)\n",
        "\n",
        "    #if self.dropout > 0:\n",
        "    #  out = F.dropout(out, self.dropout, self.training)\n",
        "\n",
        "    avg_pool = F.avg_pool2d(out, out.shape[2:])\n",
        "    max_pool = F.max_pool2d(out, out.shape[2:])\n",
        "    avg_max_pool = torch.cat((avg_pool, max_pool), 1)\n",
        "    out = avg_max_pool.view(avg_max_pool.size(0), -1)\n",
        "\n",
        "    if self.dropout > 0:\n",
        "      out = F.dropout(out, self.dropout, self.training)\n",
        "\n",
        "    out = self.fc(out)\n",
        "    out = self.output_act(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCahLVgLYmDm"
      },
      "source": [
        "# RetinaNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmOr_W4PekYW"
      },
      "source": [
        "class RetinaNetEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fpn_sizes = []\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    :param x: input tensor\n",
        "    :return: x1, x2, x3, x4 layer outputs\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiClae2uYmfz"
      },
      "source": [
        "class RetinaNet(nn.Module):\n",
        "  \n",
        "  def __init__(self, encoder: RetinaNetEncoder, num_classes, dropout_cls=0.5,\n",
        "              dropout_global_cls=0.5, use_l2_features=True):\n",
        "    super(RetinaNet, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    fpn_sizes = encoder.fpn_sizes\n",
        "    self.use_l2_features = use_l2_features\n",
        "\n",
        "    self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2], fpn_sizes[3], use_l2_features=use_l2_features)\n",
        "\n",
        "    self.regressionModel = RegressionModel(256)\n",
        "    self.classificationModel = ClassificationModel(256, num_classes=num_classes, dropout=dropout_cls)\n",
        "    self.globalClassificationModel = GlobalClassificationModel(fpn_sizes[-1], num_classes=3, feature_size=256, dropout=dropout_global_cls)\n",
        "    self.globalClassificationLoss = nn.NLLLoss()\n",
        "\n",
        "    if use_l2_features:\n",
        "      pyramid_levels = [2, 3, 4, 5, 6, 7]\n",
        "    else:\n",
        "      pyramid_levels = [3, 4, 5, 6, 7]\n",
        "\n",
        "    self.anchors = Anchors(pyramid_levels=pyramid_levels)\n",
        "\n",
        "    self.regressBoxes = BBoxTransform()\n",
        "\n",
        "    self.clipBoxes = ClipBoxes()\n",
        "\n",
        "    self.focalLoss = losses.FocalLoss()\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "    self.encoder = encoder\n",
        "\n",
        "    prior = 0.01\n",
        "\n",
        "    self.classificationModel.output.weight.data.fill_(0)\n",
        "    self.classificationModel.output.bias.data.fill_(-math.log((1.0 - prior) / prior))\n",
        "\n",
        "    self.regressionModel.output.weight.data.fill_(0)\n",
        "    self.regressionModel.output.bias.data.fill_(0)\n",
        "\n",
        "    self.freeze_bn()\n",
        "\n",
        "    def freeze_bn(self):\n",
        "      \"\"\"Freeze BatchNorm layers.\"\"\"\n",
        "      for layer in self.modules():\n",
        "        if isinstance(layer, nn.BatchNorm2d):\n",
        "          layer.eval()\n",
        "    \n",
        "    def freeze_encoder(self):\n",
        "      self.encoder.eval()\n",
        "      # correct version, but keep original as model has been trained this way\n",
        "      # for param in self.encoder.parameters():\n",
        "      #     param.requires_grad = False\n",
        "\n",
        "    def unfreeze_encoder(self):\n",
        "      for param in self.encoder.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    def boxes(self, img_batch, regression, classification, global_classification, anchors):\n",
        "      transformed_anchors = self.regressBoxes(anchors, regression)\n",
        "      transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n",
        "\n",
        "      scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
        "\n",
        "      scores_over_thresh = (scores > 0.025)[0, :, 0]\n",
        "\n",
        "      if scores_over_thresh.sum() == 0:\n",
        "        # no boxes to NMS, just return\n",
        "        return [torch.zeros(0), global_classification, torch.zeros(0, 4)]\n",
        "      else:\n",
        "        classification = classification[:, scores_over_thresh, :]\n",
        "        transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
        "        scores = scores[:, scores_over_thresh, :]\n",
        "\n",
        "        # use very low threshold of 0.05 as boxes should not overlap\n",
        "        anchors_nms_idx = nms(torch.cat([transformed_anchors, scores], dim=2)[0, :, :], 0.05)\n",
        "\n",
        "        nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n",
        "        return [nms_scores, global_classification, transformed_anchors[0, anchors_nms_idx, :]]\n",
        "\n",
        "    def forward(self, inputs, return_loss, return_boxes, return_raw=False):\n",
        "      \n",
        "      if return_loss:\n",
        "        img_batch, annotations, global_annotations = inputs\n",
        "      else:\n",
        "        img_batch = inputs\n",
        "\n",
        "      x1, x2, x3, x4 = self.encoder.forward(img_batch)\n",
        "\n",
        "      features = self.fpn([x1, x2, x3, x4])\n",
        "\n",
        "      regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n",
        "\n",
        "      classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n",
        "\n",
        "      global_classification = self.globalClassificationModel(x4)\n",
        "\n",
        "      anchors = self.anchors(img_batch)\n",
        "\n",
        "      if return_raw:\n",
        "        return [regression, classification, torch.exp(global_classification), anchors]\n",
        "\n",
        "      res = []\n",
        "\n",
        "      if return_loss:\n",
        "        res += list(self.focalLoss(classification, regression, anchors, annotations))\n",
        "        res += [self.globalClassificationLoss(global_classification, global_annotations)]\n",
        "\n",
        "      if return_boxes:\n",
        "        res += self.boxes(img_batch=img_batch,\n",
        "                          regression=regression,\n",
        "                          classification=classification,\n",
        "                          global_classification=global_classification,\n",
        "                          anchors=anchors)\n",
        "\n",
        "      return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeDVtHhNXG9_"
      },
      "source": [
        "# Defining ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbJxU406Q61-"
      },
      "source": [
        "class ResNetEncoder(RetinaNetEncoder):\n",
        "  def __init__(self, block, layers):\n",
        "    self.inplanes = 64\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "    if block == BasicBlock:\n",
        "      self.fpn_sizes = [\n",
        "        self.layer1[layers[0]-1].conv2.out_channels,\n",
        "        self.layer2[layers[1]-1].conv2.out_channels,\n",
        "        self.layer3[layers[2]-1].conv2.out_channels,\n",
        "        self.layer4[layers[3]-1].conv2.out_channels\n",
        "      ]\n",
        "    elif block == Bottleneck:\n",
        "      self.fpn_sizes = [\n",
        "        self.layer1[layers[0]-1].conv3.out_channels,\n",
        "        self.layer2[layers[1]-1].conv3.out_channels,\n",
        "        self.layer3[layers[2]-1].conv3.out_channels,\n",
        "        self.layer4[layers[3]-1].conv3.out_channels\n",
        "      ]\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "        downsample = nn.Sequential(\n",
        "          nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "            kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(planes * block.expansion),\n",
        "        )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for i in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    img_batch = inputs\n",
        "\n",
        "    x = torch.cat([img_batch, img_batch, img_batch], dim=1)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x1 = self.layer1(x)\n",
        "    x2 = self.layer2(x1)\n",
        "    x3 = self.layer3(x2)\n",
        "    x4 = self.layer4(x3)\n",
        "\n",
        "    return x1, x2, x3, x4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNVKcbAUXNza"
      },
      "source": [
        "def resnet50(num_classes, pretrained=True, **kwargs):\n",
        "  # defining a resnet50 model\n",
        "  encoder = ResNetEncoder(Bottleneck, [3,4,6,3])\n",
        "  \n",
        "  if pretrained:\n",
        "    encoder.load_state_dict(model_zoo.load_url(model_urls['https://download.pytorch.org/models/resnet50-19c8e357.pth'], model_dir='models'), strict=False)\n",
        "  \n",
        "  model = RetinaNet(encoder=encoder, num_classes=num_classes, **kwargs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPsPRD50X1ZZ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfJ9pt69Ui5y"
      },
      "source": [
        "def training_f(\n",
        "    model_name: str,\n",
        "    # fold: int,\n",
        "    # debug: bool,\n",
        "    epochs: int,\n",
        "    # run: str=None,\n",
        "    resume_weights: str=\"\",\n",
        "    resume_epoch: int=0,)\n",
        "\n",
        "  if model_name == 'resnet50':\n",
        "    retinanet = resnet50(2, pretrained)\n",
        "  \n",
        "  # TODO metti altre opzioni encoders\n",
        "\n",
        "  # TODO crea cartelle checkpoints\n",
        "  \n",
        "  # load weights to continue training\n",
        "  if resume_weights != \"\":\n",
        "    print(\"load model from: \", resume_weights)\n",
        "    retinanet = torch.load(resume_weights).cuda()\n",
        "  else:\n",
        "    retinanet = retinanet.to(device)\n",
        "\n",
        "  pretrained = True\n",
        "\n",
        "  optimizier = torch.optimizier.Adam(retinanet.parameters(), lr=1e-5)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, patience=4, verbose=True, factor=0.2\n",
        "  )\n",
        "\n",
        "  for epoch_num in range(resume_epoch+1, epochs):\n",
        "    \n",
        "    retinanet.train()\n",
        "    \n",
        "    if epoch_num < 1:\n",
        "      # train FC layers with freezed encoder for the first epoch\n",
        "      retinanet.module.freeze_encoder()  \n",
        "    else:\n",
        "      retinanet.module.unfreeze_encoder()\n",
        "    \n",
        "    retinanet.module.freeze_bn()\n",
        "\n",
        "    # losses\n",
        "    epoch_loss, loss_cls_hist, loss_cls_global_hist, loss_reg_hist = [], [], [], []\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "      data_iter = tqdm(enumerate(dataloader_train), total = len(dataloader_train))\n",
        "      for iter_num, data in data_iter:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = [\n",
        "                  data['img'].cuda().float(),\n",
        "                  data['annot'].cuda().float(),\n",
        "                  data['category'].cuda(),\n",
        "        ]\n",
        "        (classification_loss, regression_loss, global_classification_loss,) = retinanet(\n",
        "            inputs, return_loss=True, return_boxes=False\n",
        "            )\n",
        "        \n",
        "        classification_loss = classification_loss.mean() \n",
        "        regression_loss = regression_loss.mean()\n",
        "        global_classification_loss = global_classification_loss.mean()\n",
        "        loss = classification_loss + regression_loss + global_classification_loss*0.1\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.05)\n",
        "        optimizer.step()\n",
        "        # loss history\n",
        "        loss_cls_hist.append(float(classification_loss))\n",
        "        loss_cls_global_hist.append(float(global_classification_loss))\n",
        "        loss_reg_hist.append(float(regression_loss))\n",
        "        epoch_loss.append(float(loss))\n",
        "        # print losses with tqdm interator\n",
        "        data_iter.set_description(\n",
        "          f\"{epoch_num} cls: {np.mean(loss_cls_hist):1.4f} cls g: {np.mean(loss_cls_global_hist):1.4f} Reg: {np.mean(loss_reg_hist):1.4f} Loss: {np.mean(epoch_loss):1.4f}\"\n",
        "        )\n",
        "        del classification_loss\n",
        "        del regression_loss\n",
        "\n",
        "  # TODO save model and log loss history\n",
        "  \n",
        "\n",
        "  # validation\n",
        "  (\n",
        "    loss_hist_valid,\n",
        "    loss_cls_hist_valid,\n",
        "    loss_cls_global_hist_valid,\n",
        "    loss_reg_hist_valid,\n",
        "  ) = validation(retinanet,\n",
        "        dataloader_valid,\n",
        "        epoch_num,\n",
        "        predictions_dir,\n",
        "        save_oof=True,\n",
        "  )\n",
        "  \n",
        "  # log validation loss history\n",
        "  logger.scalar_summary(\"loss_valid\", np.mean(loss_hist_valid), epoch_num)\n",
        "  logger.scalar_summary(\"loss_valid_classification\", np.mean(loss_cls_hist_valid), epoch_num)\n",
        "  logger.scalar_summary(\n",
        "    \"loss_valid_global_classification\", np.mean(loss_cls_global_hist_valid), epoch_num,\n",
        "  )\n",
        "  logger.scalar_summary(\"loss_valid_regression\", np.mean(loss_reg_hist_valid), epoch_num)\n",
        "  \n",
        "  scheduler.step(np.mean(loss_reg_hist_valid))\n",
        "  retinanet.eval()\n",
        "\n",
        "  # TODO riscrivi percorso cartella\n",
        "  torch.save(retinanet, f\"{checkpoints_dir}/{model_name}_final.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvyFOZop1zd_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}