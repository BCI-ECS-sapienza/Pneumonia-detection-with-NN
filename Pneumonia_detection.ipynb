{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pneumonia_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7ddd8ece0b245fab954d77f24292604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b755b4ebdd674c56afb047fc5bc3d581",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e80f9d90fd2437f9c718b5fd023905e",
              "IPY_MODEL_19af2fc9c1164960bf5a7bb130ef37ba"
            ]
          }
        },
        "b755b4ebdd674c56afb047fc5bc3d581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e80f9d90fd2437f9c718b5fd023905e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cb06a7325a674c8b89e3222413832e54",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a9e3dc2d3ea46ebbbf5c3ca32e3fe9a"
          }
        },
        "19af2fc9c1164960bf5a7bb130ef37ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ca03cf6e40941699e7e331375985ae6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 228MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a86938d765a1409a875859a146d236ae"
          }
        },
        "cb06a7325a674c8b89e3222413832e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a9e3dc2d3ea46ebbbf5c3ca32e3fe9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ca03cf6e40941699e7e331375985ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a86938d765a1409a875859a146d236ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1L0Amxga2WV"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8al2L_HFLqjD",
        "outputId": "666c5c5d-028a-4c33-ee49-802d988509b0"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axdYCP7K-Sww",
        "outputId": "2343301f-9321-4f6f-f139-3a65e9c92527"
      },
      "source": [
        "pip install pretrainedmodels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.7/dist-packages (0.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (2.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.9.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhtv_XbiS4iM",
        "outputId": "fdd3b20c-3af1-48ad-ed8c-8a8b264b675f"
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoJFTUBEaqQ3"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from pretrainedmodels.models import senet, pnasnet, xception"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRvf2fDqsiza"
      },
      "source": [
        "# Setting Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt-F4C1xsi-d"
      },
      "source": [
        "# setting gpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TIFC8iN-flo"
      },
      "source": [
        "# Load Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7g46iWTafn"
      },
      "source": [
        "Let's load the folders and increase the data for each bouinding box with 'x_max' and 'y_max', in order to adopt the Pascal standard format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_daCeAK6tZ9",
        "outputId": "a85e2d4a-5fb9-4d24-f7da-5f6164ef9fa5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "IMAGES_DIR = '/content/drive/My Drive/Neural/dataset/stage_2_train_images'\n",
        "LABELS_CSV = '/content/drive/My Drive/Neural/dataset/stage_2_train_labels.csv'\n",
        "CLASS_INFO_CSV = '/content/drive/My Drive/Neural/dataset/stage_2_detailed_class_info.csv'\n",
        "\n",
        "print(IMAGES_DIR)\n",
        "print(LABELS_CSV)\n",
        "print(CLASS_INFO_CSV)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Neural/dataset/stage_2_train_images\n",
            "/content/drive/My Drive/Neural/dataset/stage_2_train_labels.csv\n",
            "/content/drive/My Drive/Neural/dataset/stage_2_detailed_class_info.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "sEzoQP3HkSj1",
        "outputId": "32d1dc3b-c268-4961-80ca-fb5548e0d70f"
      },
      "source": [
        "train_labels_df = pd.read_csv(LABELS_CSV)\n",
        "class_info_df = pd.read_csv(CLASS_INFO_CSV)\n",
        "train_class_df = train_labels_df.merge(class_info_df, left_on='patientId', right_on='patientId', how='inner')\n",
        "print(train_class_df.count())\n",
        "train_class_df[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patientId    37629\n",
            "x            16957\n",
            "y            16957\n",
            "width        16957\n",
            "height       16957\n",
            "Target       37629\n",
            "class        37629\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patientId</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>Target</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
              "      <td>264.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
              "      <td>264.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
              "      <td>562.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>453.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
              "      <td>562.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>453.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00569f44-917d-4c86-a842-81832af98c30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>006cec2e-6ce2-4549-bffa-eadfcd1e9970</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              patientId  ...                         class\n",
              "0  0004cfab-14fd-4e49-80ba-63a80b6bddd6  ...  No Lung Opacity / Not Normal\n",
              "1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd  ...  No Lung Opacity / Not Normal\n",
              "2  00322d4d-1c29-4943-afc9-b6754be640eb  ...  No Lung Opacity / Not Normal\n",
              "3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5  ...                        Normal\n",
              "4  00436515-870c-4b36-a041-de91049b9ab4  ...                  Lung Opacity\n",
              "5  00436515-870c-4b36-a041-de91049b9ab4  ...                  Lung Opacity\n",
              "6  00436515-870c-4b36-a041-de91049b9ab4  ...                  Lung Opacity\n",
              "7  00436515-870c-4b36-a041-de91049b9ab4  ...                  Lung Opacity\n",
              "8  00569f44-917d-4c86-a842-81832af98c30  ...  No Lung Opacity / Not Normal\n",
              "9  006cec2e-6ce2-4549-bffa-eadfcd1e9970  ...  No Lung Opacity / Not Normal\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tvg1rDSlrpM"
      },
      "source": [
        "Needs to remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "lm8-Y48blIDr",
        "outputId": "4bbee83b-046a-4d18-a90d-bba87d5417a1"
      },
      "source": [
        "train_class_df = train_class_df.drop_duplicates()\n",
        "print(train_class_df.count())\n",
        "train_class_df[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patientId    30227\n",
            "x             9555\n",
            "y             9555\n",
            "width         9555\n",
            "height        9555\n",
            "Target       30227\n",
            "class        30227\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patientId</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>Target</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
              "      <td>264.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
              "      <td>562.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>453.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00569f44-917d-4c86-a842-81832af98c30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>006cec2e-6ce2-4549-bffa-eadfcd1e9970</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>00704310-78a8-4b38-8475-49f4573b2dbb</td>\n",
              "      <td>323.0</td>\n",
              "      <td>577.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>00704310-78a8-4b38-8475-49f4573b2dbb</td>\n",
              "      <td>695.0</td>\n",
              "      <td>575.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               patientId  ...                         class\n",
              "0   0004cfab-14fd-4e49-80ba-63a80b6bddd6  ...  No Lung Opacity / Not Normal\n",
              "1   00313ee0-9eaa-42f4-b0ab-c148ed3241cd  ...  No Lung Opacity / Not Normal\n",
              "2   00322d4d-1c29-4943-afc9-b6754be640eb  ...  No Lung Opacity / Not Normal\n",
              "3   003d8fa0-6bf1-40ed-b54c-ac657f8495c5  ...                        Normal\n",
              "4   00436515-870c-4b36-a041-de91049b9ab4  ...                  Lung Opacity\n",
              "6   00436515-870c-4b36-a041-de91049b9ab4  ...                  Lung Opacity\n",
              "8   00569f44-917d-4c86-a842-81832af98c30  ...  No Lung Opacity / Not Normal\n",
              "9   006cec2e-6ce2-4549-bffa-eadfcd1e9970  ...  No Lung Opacity / Not Normal\n",
              "10  00704310-78a8-4b38-8475-49f4573b2dbb  ...                  Lung Opacity\n",
              "12  00704310-78a8-4b38-8475-49f4573b2dbb  ...                  Lung Opacity\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "lKznWS68TbDw",
        "outputId": "b109a182-3948-4d6c-d2d7-ec9b4931ff7e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# fill empty labels (class 0) with all 0\n",
        "train_class_df.x.fillna(0, inplace=True)\n",
        "train_class_df.y.fillna(0, inplace=True)\n",
        "train_class_df.width.fillna(0, inplace=True)\n",
        "train_class_df.height.fillna(0, inplace=True)\n",
        "\n",
        "# add Pascal bounding boxes encoding\n",
        "train_class_df['x_max'] = train_class_df['x']+train_class_df['width']\n",
        "train_class_df['y_max'] = train_class_df['y']+train_class_df['height']\n",
        "\n",
        "train_class_df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patientId</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>Target</th>\n",
              "      <th>class</th>\n",
              "      <th>x_max</th>\n",
              "      <th>y_max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2247</th>\n",
              "      <td>17dc4e24-2228-4cdb-8052-9259a907f9c2</td>\n",
              "      <td>557.0</td>\n",
              "      <td>350.0</td>\n",
              "      <td>293.0</td>\n",
              "      <td>478.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "      <td>850.0</td>\n",
              "      <td>828.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16187</th>\n",
              "      <td>8355ac5a-924d-4c43-99e0-f675b5f0e86c</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35284</th>\n",
              "      <td>fbe472aa-1dda-4c4d-af44-2d41966c819d</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27247</th>\n",
              "      <td>c2d86046-7af9-4019-b130-9e9bfde1abfe</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15577</th>\n",
              "      <td>7f41d711-55ff-44a5-8857-b02eb300174b</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21538</th>\n",
              "      <td>a694ac65-d011-4e71-b6f8-653258278b3f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19927</th>\n",
              "      <td>9bb53dc0-28ed-47ed-bff3-9e6cc3c03d62</td>\n",
              "      <td>650.0</td>\n",
              "      <td>473.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "      <td>925.0</td>\n",
              "      <td>798.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12999</th>\n",
              "      <td>6e88a38a-9467-4ab9-b517-3664e98bc42b</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37284</th>\n",
              "      <td>2bf8a7ed-b4a5-4ea1-b8e8-cadc7f7adf2d</td>\n",
              "      <td>179.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>523.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "      <td>432.0</td>\n",
              "      <td>777.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18626</th>\n",
              "      <td>92a0f940-652a-4a9e-aeee-26f003abda25</td>\n",
              "      <td>147.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>738.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Lung Opacity</td>\n",
              "      <td>417.0</td>\n",
              "      <td>1014.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  patientId      x  ...  x_max   y_max\n",
              "2247   17dc4e24-2228-4cdb-8052-9259a907f9c2  557.0  ...  850.0   828.0\n",
              "16187  8355ac5a-924d-4c43-99e0-f675b5f0e86c    0.0  ...    0.0     0.0\n",
              "35284  fbe472aa-1dda-4c4d-af44-2d41966c819d    0.0  ...    0.0     0.0\n",
              "27247  c2d86046-7af9-4019-b130-9e9bfde1abfe    0.0  ...    0.0     0.0\n",
              "15577  7f41d711-55ff-44a5-8857-b02eb300174b    0.0  ...    0.0     0.0\n",
              "21538  a694ac65-d011-4e71-b6f8-653258278b3f    0.0  ...    0.0     0.0\n",
              "19927  9bb53dc0-28ed-47ed-bff3-9e6cc3c03d62  650.0  ...  925.0   798.0\n",
              "12999  6e88a38a-9467-4ab9-b517-3664e98bc42b    0.0  ...    0.0     0.0\n",
              "37284  2bf8a7ed-b4a5-4ea1-b8e8-cadc7f7adf2d  179.0  ...  432.0   777.0\n",
              "18626  92a0f940-652a-4a9e-aeee-26f003abda25  147.0  ...  417.0  1014.0\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8qATuuMTLxC"
      },
      "source": [
        "## Augmentations definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orp68nNITD98"
      },
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
        "\n",
        "def augmentation_pipeline(level):\n",
        "  if level == 'resize_only':\n",
        "    list_augmentations = [\n",
        "      iaa.Resize(512)            \n",
        "    ]\n",
        "\n",
        "  elif level == 'light':\n",
        "    list_augmentations = [\n",
        "      iaa.Resize(512),\n",
        "      iaa.Affine(\n",
        "        scale=1.1, \n",
        "        shear=(2.5,2.5), \n",
        "        rotate=(-5, 5), \n",
        "      ),    \n",
        "    ]\n",
        "    \n",
        "  elif level == 'heavy': #no rotation included\n",
        "    list_augmentations = [\n",
        "      iaa.Resize(512),\n",
        "      iaa.Affine(\n",
        "        scale=1.15, \n",
        "        shear=(4.0,4.0),\n",
        "      ),   \n",
        "      iaa.Fliplr(0.2), # horizontally flip 20% of the images\n",
        "      iaa.Sometimes(0.1, iaa.CoarseSaltAndPepper(p=(0.01, 0.01), size_percent=(0.1, 0.2))),\n",
        "      iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0.0, 2.0))),\n",
        "      iaa.Sometimes(0.5, iaa.AdditiveGaussianNoise(scale=(0, 0.04 * 255))),            \n",
        "           \n",
        "    ]\n",
        "\n",
        "  elif level == 'heavy_with_rotations':\n",
        "    list_augmentations = [\n",
        "      iaa.Resize(512),\n",
        "      iaa.Affine(\n",
        "        scale=1.15, \n",
        "        shear=(4.0,4.0),\n",
        "        rotate=(-6, 6), \n",
        "      ),   \n",
        "      iaa.Fliplr(0.2), # horizontally flip 20% of the images\n",
        "      iaa.Sometimes(0.1, iaa.CoarseSaltAndPepper(p=(0.01, 0.01), size_percent=(0.1, 0.2))),\n",
        "      iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0.0, 2.0))),\n",
        "      iaa.Sometimes(0.5, iaa.AdditiveGaussianNoise(scale=(0, 0.04 * 255))),            \n",
        "    ]\n",
        "\n",
        "  return list_augmentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_fs9cUWTNW1"
      },
      "source": [
        "## Custom dataset definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kutfOSsi55BX"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import pydicom\n",
        "\n",
        "\n",
        "def get_image_array(image_path):\n",
        "  try:\n",
        "    dcm_data = pydicom.read_file(image_path)\n",
        "    img = dcm_data.pixel_array\n",
        "    return img\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "def parse_one_annot(box_coord, filename):\n",
        "  boxes_array = box_coord[box_coord[\"patientId\"] == filename][[\"x\", \"y\", \"x_max\", \"y_max\"]].values\n",
        "  return boxes_array "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m06YIrUZ6lzP"
      },
      "source": [
        "class CXRimages(Dataset):\n",
        "    def __init__(self, csv_file, images_dir, augmentations='resize_only', transform=None):\n",
        "      self.path = images_dir      \n",
        "      self.annotations = csv_file\n",
        "      self.categories = [\"No Lung Opacity / Not Normal\", \"Normal\", \"Lung Opacity\"]\n",
        "      self.augmentations = augmentation_pipeline(augmentations)    # augmentations with imgaug\n",
        "      self.transform = transform                                   # Images ToTensor and normalize\n",
        "      #self.imgs = sorted(os.listdir(images_dir))\n",
        "\n",
        "\n",
        "    def num_classes(self):\n",
        "      return 3\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.annotations)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):   # requires to define new indexes from 0\n",
        "        patient_id = self.annotations['patientId'][idx]\n",
        "        category = self.categories.index(self.annotations['class'][idx])\n",
        "        target = self.annotations['Target'][idx]\n",
        "\n",
        "        # load image\n",
        "        img_path = os.path.join(self.path, patient_id +'.dcm')\n",
        "        img = get_image_array(img_path)  \n",
        "        image = img / 255.0\n",
        "\n",
        "        image = (255*image).clip(0, 255).astype(np.uint8)\n",
        "        image = Image.fromarray(image).convert('RGB')\n",
        "  \n",
        "\n",
        "        # get bounding boxes from csv\n",
        "        box_list = parse_one_annot(self.annotations, patient_id)\n",
        "        boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
        "        num_objs = len(box_list)\n",
        "\n",
        "        # get box encoding for imaug\n",
        "        list_boxes = []\n",
        "        for j in range(num_objs):\n",
        "          list_boxes.append(BoundingBox(x1=boxes[j][0].item(), x2=boxes[j][2].item(), y1=boxes[j][1].item(), y2=boxes[j][3].item()))\n",
        "        bbs = BoundingBoxesOnImage(list_boxes, shape=img.shape)\n",
        "\n",
        "        # augmentation\n",
        "        seq_training = iaa.Sequential(self.augmentations)\n",
        "        image_aug, bbs_aug = seq_training(image=img, bounding_boxes=bbs)     \n",
        "\n",
        "        # set bounding boxes on required encoding for the model\n",
        "        final_boxes = np.zeros((0, 5))\n",
        "\n",
        "        if target == 1:\n",
        "          for box in bbs_aug.bounding_boxes:\n",
        "            annotation  = np.zeros((1, 5))\n",
        "            annotation[0, :4] = [box.x1, box.y1, box.x2, box.y2]\n",
        "            annotation[0, 4]  = target\n",
        "            final_boxes       = np.append(final_boxes, annotation, axis=0)  \n",
        "          \n",
        "          final_boxes = np.row_stack(final_boxes)\n",
        "\n",
        "\n",
        "        if self.transform is not None:\n",
        "                image_aug = self.transform(image_aug.copy())  # .copy() avoid negative values in tensor\n",
        "\n",
        "        output = {\"img\": image_aug, \"annot\": final_boxes, \"scale\": 1.0, 'category': category}\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmwzMe9CT4Vd"
      },
      "source": [
        "## Split - Training - Validation - Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An50HGjHTWJD",
        "outputId": "41e81a4f-e478-4b84-e4b6-f1049697815c"
      },
      "source": [
        "np.random.seed(13)\n",
        "msk = np.random.rand(len(train_class_df)) < 0.8\n",
        "\n",
        "# split train and val/test + add indexes from 0 as required by class definition\n",
        "train_df = train_class_df[msk].reset_index()  \n",
        "val_train_df = train_class_df[~msk]\n",
        "\n",
        "# aplit val/test\n",
        "split_val = int(len(val_train_df)/2)\n",
        "val_df = val_train_df.iloc[:split_val,:].reset_index()\n",
        "test_df = val_train_df.iloc[split_val:,:].reset_index()\n",
        "\n",
        "print(f'Samples in train set: {len(train_df)} \\nSamples in validation set: {len(val_df)} \\nSamples in test set: {len(val_df)} \\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples in train set: 24085 \n",
            "Samples in validation set: 3071 \n",
            "Samples in test set: 3071 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSfdjHebUD60"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWpAH8vzUUfL"
      },
      "source": [
        "Remember to set the correct augmentation for the training set! You can select between:\n",
        "- 'resize_only'\n",
        "- 'light'\n",
        "- 'heavy'\n",
        "- 'heavy_with_rotations'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PccLlNHfUClh",
        "outputId": "860ac925-ae79-4581-a7a1-306318aeb6f0"
      },
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "def To_tensor_tfms():\n",
        "   transforms = []\n",
        "   transforms.append(T.ToTensor())\n",
        "   return T.Compose(transforms)\n",
        "\n",
        "train_dataset = CXRimages(csv_file = train_df , images_dir = IMAGES_DIR, augmentations='resize_only', transform = None)\n",
        "\n",
        "val_dataset = CXRimages(csv_file = val_df , images_dir = IMAGES_DIR, augmentations='resize_only', transform = None)\n",
        "test_dataset = CXRimages(csv_file = test_df , images_dir = IMAGES_DIR, augmentations='resize_only', transform = None)\n",
        "\n",
        "print(f'Samples in train set: {len(train_dataset)} \\nSamples in validation set: {len(val_dataset)} \\nSamples in test set: {len(test_dataset)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples in train set: 24085 \n",
            "Samples in validation set: 3071 \n",
            "Samples in test set: 3071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkHFnnwfFeHb"
      },
      "source": [
        "def collater2d(data):\n",
        "    imgs = [s['img'] for s in data]\n",
        "    annots = [s['annot'] for s in data]\n",
        "    scales = [s['scale'] for s in data]\n",
        "    cats = np.array([s['category'] for s in data])\n",
        "\n",
        "    widths = [int(s.shape[0]) for s in imgs]\n",
        "    heights = [int(s.shape[1]) for s in imgs]\n",
        "    batch_size = len(imgs)\n",
        "\n",
        "    max_width = np.array(widths).max()\n",
        "    max_height = np.array(heights).max()\n",
        "\n",
        "    padded_imgs = torch.zeros(batch_size, max_width, max_height, 1)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        img = imgs[i]\n",
        "        padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), 0] = torch.from_numpy(img)\n",
        "\n",
        "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
        "\n",
        "    if max_num_annots > 0:\n",
        "\n",
        "        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
        "\n",
        "        if max_num_annots > 0:\n",
        "            for idx, annot in enumerate(annots):\n",
        "                # print(annot.shape)\n",
        "                if annot.shape[0] > 0:\n",
        "                    annot_padded[idx, :annot.shape[0], :] = torch.from_numpy(annot)\n",
        "    else:\n",
        "        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
        "\n",
        "    padded_imgs = padded_imgs.permute(0, 3, 1, 2)\n",
        "\n",
        "    return {'img': padded_imgs, 'annot': annot_padded, 'scale': scales, 'category': torch.from_numpy(cats)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ycf5-g-K1gT"
      },
      "source": [
        "# set batch size\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collater2d) \n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collater2d)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collater2d) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROZ8KO5i-mDo"
      },
      "source": [
        "# RetinaNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj4VSazaIdVX"
      },
      "source": [
        "## Anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw_xlM9XIfKU"
      },
      "source": [
        "class Anchors(nn.Module):\n",
        "  def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n",
        "    super(Anchors, self).__init__()\n",
        "\n",
        "    if pyramid_levels is None:\n",
        "      self.pyramid_levels = [3, 4, 5, 6, 7]\n",
        "    else:\n",
        "      self.pyramid_levels = pyramid_levels\n",
        "\n",
        "    if strides is None:\n",
        "      self.strides = [2 ** x for x in self.pyramid_levels]\n",
        "    if sizes is None:\n",
        "      self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
        "    if ratios is None:\n",
        "      self.ratios = np.array([0.5, 1, 2])\n",
        "    if scales is None:\n",
        "      self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
        "\n",
        "  def forward(self, image):\n",
        "        \n",
        "    image_shape = image.shape[2:]\n",
        "    image_shape = np.array(image_shape)\n",
        "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
        "\n",
        "    # compute anchors over all pyramid levels\n",
        "    all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
        "\n",
        "    for idx, p in enumerate(self.pyramid_levels):\n",
        "      anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
        "      shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
        "      all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
        "\n",
        "    all_anchors = np.expand_dims(all_anchors, axis=0)\n",
        "\n",
        "    return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n",
        "  \n",
        "\n",
        "def generate_anchors(base_size=16, ratios=None, scales=None):\n",
        "    \"\"\"\n",
        "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
        "    scales w.r.t. a reference window.\n",
        "    \"\"\"\n",
        "\n",
        "    if ratios is None:\n",
        "      ratios = np.array([0.5, 1, 2])\n",
        "\n",
        "    if scales is None:\n",
        "      scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
        "\n",
        "    num_anchors = len(ratios) * len(scales)\n",
        "\n",
        "    # initialize output anchors\n",
        "    anchors = np.zeros((num_anchors, 4))\n",
        "\n",
        "    # scale base_size\n",
        "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
        "\n",
        "    # compute areas of anchors\n",
        "    areas = anchors[:, 2] * anchors[:, 3]\n",
        "\n",
        "    # correct for ratios\n",
        "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
        "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
        "\n",
        "    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
        "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
        "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
        "\n",
        "    return anchors\n",
        "  \n",
        "def compute_shape(image_shape, pyramid_levels):\n",
        "  \"\"\"Compute shapes based on pyramid levels.\n",
        "  :param image_shape:\n",
        "  :param pyramid_levels:\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  image_shape = np.array(image_shape[:2])\n",
        "  image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
        "  return image_shape\n",
        "\n",
        "def anchors_for_shape(\n",
        "  image_shape,\n",
        "  pyramid_levels=None,\n",
        "  ratios=None,\n",
        "  scales=None,\n",
        "  strides=None,\n",
        "  sizes=None,\n",
        "  shapes_callback=None,\n",
        "  ):\n",
        "\n",
        "  image_shapes = compute_shape(image_shape, pyramid_levels)\n",
        "\n",
        "  # compute anchors over all pyramid levels\n",
        "  all_anchors = np.zeros((0, 4))\n",
        "  for idx, p in enumerate(pyramid_levels):\n",
        "    anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n",
        "    shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n",
        "    all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
        "\n",
        "  return all_anchors\n",
        "\n",
        "def shift(shape, stride, anchors):\n",
        "  shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
        "  shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
        "\n",
        "  shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "\n",
        "  shifts = np.vstack((\n",
        "      shift_x.ravel(), shift_y.ravel(),\n",
        "      shift_x.ravel(), shift_y.ravel()\n",
        "  )).transpose()\n",
        "\n",
        "  # add A anchors (1, A, 4) to\n",
        "  # cell K shifts (K, 1, 4) to get\n",
        "  # shift anchors (K, A, 4)\n",
        "  # reshape to (K*A, 4) shifted anchors\n",
        "  A = anchors.shape[0]\n",
        "  K = shifts.shape[0]\n",
        "  all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
        "  all_anchors = all_anchors.reshape((K * A, 4))\n",
        "\n",
        "  return all_anchors\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG5KlFggJf8y"
      },
      "source": [
        "## Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2a1qDhKJiHO"
      },
      "source": [
        "class BBoxTransform(nn.Module):\n",
        "\n",
        "  def __init__(self, mean=None, std=None):\n",
        "    super(BBoxTransform, self).__init__()\n",
        "    if mean is None:\n",
        "      self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32)).cuda()\n",
        "    else:\n",
        "      self.mean = mean\n",
        "    if std is None:\n",
        "      self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32)).cuda()\n",
        "    else:\n",
        "      self.std = std\n",
        "  \n",
        "  def forward(self, boxes, deltas):\n",
        "\n",
        "    widths  = boxes[:, :, 2] - boxes[:, :, 0]\n",
        "    heights = boxes[:, :, 3] - boxes[:, :, 1]\n",
        "    ctr_x   = boxes[:, :, 0] + 0.5 * widths\n",
        "    ctr_y   = boxes[:, :, 1] + 0.5 * heights\n",
        "\n",
        "    dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n",
        "    dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n",
        "    dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n",
        "    dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n",
        "\n",
        "    pred_ctr_x = ctr_x + dx * widths\n",
        "    pred_ctr_y = ctr_y + dy * heights\n",
        "    pred_w     = torch.exp(dw) * widths\n",
        "    pred_h     = torch.exp(dh) * heights\n",
        "\n",
        "    pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n",
        "    pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n",
        "    pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n",
        "    pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n",
        "\n",
        "    pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n",
        "\n",
        "    return pred_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcxhjsivJ1Bg"
      },
      "source": [
        "class ClipBoxes(nn.Module):\n",
        "\n",
        "  def __init__(self, width=None, height=None):\n",
        "    super(ClipBoxes, self).__init__()\n",
        "\n",
        "  def forward(self, boxes, img):\n",
        "\n",
        "    batch_size, num_channels, height, width = img.shape\n",
        "\n",
        "    boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n",
        "    boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n",
        "\n",
        "    boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n",
        "    boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n",
        "      \n",
        "    return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hho5FZuf-1D6"
      },
      "source": [
        "## Encoders\n",
        "Choose one of the following encoders as the backbone for the RetinNet model. The output of the encoders is the input for the FPN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmOr_W4PekYW"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fpn_sizes = []\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    :param x: input tensor\n",
        "    :return: x1, x2, x3, x4 layer outputs\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jDJk-0x_P0_"
      },
      "source": [
        "### ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEJaaEjsb6Zk"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      residual = self.downsample(x)\n",
        "\n",
        "    out += residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m86ovbtKYSI1"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn3(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      residual = self.downsample(x)\n",
        "\n",
        "    out += residual\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbJxU406Q61-"
      },
      "source": [
        "class ResNet(Encoder):\n",
        "  def __init__(self, block, layers):\n",
        "    self.inplanes = 64\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "    if block == BasicBlock:\n",
        "      self.fpn_sizes = [\n",
        "        self.layer1[layers[0]-1].conv2.out_channels,\n",
        "        self.layer2[layers[1]-1].conv2.out_channels,\n",
        "        self.layer3[layers[2]-1].conv2.out_channels,\n",
        "        self.layer4[layers[3]-1].conv2.out_channels\n",
        "      ]\n",
        "    elif block == Bottleneck:\n",
        "      self.fpn_sizes = [\n",
        "        self.layer1[layers[0]-1].conv3.out_channels,\n",
        "        self.layer2[layers[1]-1].conv3.out_channels,\n",
        "        self.layer3[layers[2]-1].conv3.out_channels,\n",
        "        self.layer4[layers[3]-1].conv3.out_channels\n",
        "      ]\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "        downsample = nn.Sequential(\n",
        "          nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "            kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(planes * block.expansion),\n",
        "        )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for i in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    img_batch = inputs\n",
        "\n",
        "    x = torch.cat([img_batch, img_batch, img_batch], dim=1)\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x1 = self.layer1(x)\n",
        "    x2 = self.layer2(x1)\n",
        "    x3 = self.layer3(x2)\n",
        "    x4 = self.layer4(x3)\n",
        "\n",
        "    return x1, x2, x3, x4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNVKcbAUXNza"
      },
      "source": [
        "def resnet50(num_classes, pretrained=True, **kwargs):\n",
        "  # defining a resnet50 model\n",
        "  encoder = ResNet(Bottleneck, [3,4,6,3])\n",
        "  \n",
        "  if pretrained:\n",
        "    #encoder.load_state_dict(model_zoo.load_url(model_urls['https://download.pytorch.org/models/resnet50-19c8e357.pth'], model_dir='models'), strict=False)\n",
        "    encoder.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth'), strict=False)\n",
        "  model = RetinaNet(encoder=encoder, num_classes=num_classes, **kwargs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZcMnkBh_jKj"
      },
      "source": [
        "### SE-ResNeXt50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AajC7ATU_mMQ"
      },
      "source": [
        "class SElayer(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes):\n",
        "        super(SElayer, self).__init__()\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv1 = nn.Conv2d(inplanes, inplanes / 16, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(inplanes / 16, inplanes, kernel_size=1, stride=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.global_avgpool(x)\n",
        "\n",
        "        out = self.conv1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return x * out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Duz5dg2_oiT"
      },
      "source": [
        "class BottleneckX(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n",
        "        super(BottleneckX, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n",
        "                               padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "\n",
        "        self.selayer = SElayer(planes * 4)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out = self.selayer(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5SbXuX9_rB4"
      },
      "source": [
        "class SEResNeXt(Encoder):\n",
        "\n",
        "    def __init__(self, block, layers, cardinality=32, num_classes=1000):\n",
        "        super(SEResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.inplanes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, self.cardinality, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, self.cardinality))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x1 = self.layer1(x)\n",
        "        x2 = self.layer2(x1)\n",
        "        x3 = self.layer3(x2)\n",
        "        x4 = self.layer4(x3)\n",
        "\n",
        "        return x1, x2, x3, x4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql0-jLZJ_trD"
      },
      "source": [
        "def se_resnext50(**kwargs):\n",
        "    \"\"\"Constructs a SE-ResNeXt-50 model.\n",
        "    Args:\n",
        "        num_classes = 1000 (default)\n",
        "    \"\"\"\n",
        "    encoder = SEResNeXt(BottleneckX, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained == 'imagenet':\n",
        "        encoder.load_state_dict(model_zoo.load_url(\n",
        "            senet.pretrained_settings['se_resnext50_32x4d']['imagenet']['url'], model_dir='models'), strict=False)\n",
        "\n",
        "    model = RetinaNet(encoder=encoder, num_classes=num_classes, dropout_cls=dropout, dropout_global_cls=dropout)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asztcuOm_z5A"
      },
      "source": [
        "### PNasnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltxQaxyw_zSW"
      },
      "source": [
        "class PNasnet(Encoder):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(PNasnet, self).__init__()\n",
        "        self.encoder = pnasnet.PNASNet5Large(num_classes=1001)\n",
        "\n",
        "        self.fpn_sizes = [270, 1080, 2160, 4320]\n",
        "        print(self.fpn_sizes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = torch.cat([inputs, inputs, inputs], dim=1)\n",
        "        x_conv_0 = self.encoder.conv_0(x)\n",
        "        x_stem_0 = self.encoder.cell_stem_0(x_conv_0)\n",
        "        x_stem_1 = self.encoder.cell_stem_1(x_conv_0, x_stem_0)\n",
        "        x_cell_0 = self.encoder.cell_0(x_stem_0, x_stem_1)\n",
        "        x_cell_1 = self.encoder.cell_1(x_stem_1, x_cell_0)\n",
        "        x_cell_2 = self.encoder.cell_2(x_cell_0, x_cell_1)\n",
        "        x_cell_3 = self.encoder.cell_3(x_cell_1, x_cell_2)\n",
        "        x_cell_4 = self.encoder.cell_4(x_cell_2, x_cell_3)\n",
        "        x_cell_5 = self.encoder.cell_5(x_cell_3, x_cell_4)\n",
        "        x_cell_6 = self.encoder.cell_6(x_cell_4, x_cell_5)\n",
        "        x_cell_7 = self.encoder.cell_7(x_cell_5, x_cell_6)\n",
        "        x_cell_8 = self.encoder.cell_8(x_cell_6, x_cell_7)\n",
        "        x_cell_9 = self.encoder.cell_9(x_cell_7, x_cell_8)\n",
        "        x_cell_10 = self.encoder.cell_10(x_cell_8, x_cell_9)\n",
        "        x_cell_11 = self.encoder.cell_11(x_cell_9, x_cell_10)\n",
        "\n",
        "        return x_stem_0, x_cell_3, x_cell_7, x_cell_11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC0uf7fA_5_n"
      },
      "source": [
        "def pnasnet5(num_classes, pretrained=True, dropout=0.0):\n",
        "    \"\"\"Constructs a DPN model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    encoder = PNasnet()\n",
        "\n",
        "    if pretrained:\n",
        "        encoder.encoder.load_state_dict(model_zoo.load_url(\n",
        "            pnasnet.pretrained_settings['pnasnet5large']['imagenet+background']['url'], model_dir='models'), strict=False)\n",
        "\n",
        "    model = RetinaNet(encoder=encoder, num_classes=num_classes, dropout_cls=dropout, dropout_global_cls=dropout)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mAptUla_9r1"
      },
      "source": [
        "### Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wups6k5__FC"
      },
      "source": [
        "class Xception(Encoder):\n",
        "    \"\"\"\n",
        "    Xception optimized for the ImageNet dataset: https://arxiv.org/pdf/1610.02357.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            num_classes: number of classes\n",
        "        \"\"\"\n",
        "        super(Xception, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, bias=False, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        # relu\n",
        "\n",
        "        self.block1 = Block(64, 128, 2, 2, start_with_relu=False, grow_first=True)\n",
        "        self.block2 = Block(128, 256, 2, 2, start_with_relu=True, grow_first=True)\n",
        "        self.block3 = Block(256, 728, 2, 2, start_with_relu=True, grow_first=True)\n",
        "\n",
        "        self.block4 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "        self.block5 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "        self.block6 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "        self.block7 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "\n",
        "        self.block8 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "        self.block9 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "        self.block10 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "        self.block11 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
        "\n",
        "        self.block12 = Block(728, 1024, 2, 2, start_with_relu=True, grow_first=False)\n",
        "\n",
        "        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n",
        "        self.bn3 = nn.BatchNorm2d(1536)\n",
        "\n",
        "        # relu\n",
        "        self.conv4 = SeparableConv2d(1536, 2048, 3, 1, 1)\n",
        "        self.bn4 = nn.BatchNorm2d(2048)\n",
        "\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "        self.fpn_sizes = [128, 256, 728, 2048]\n",
        "        print(self.fpn_sizes)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        res = []\n",
        "\n",
        "        x = torch.cat([inputs, inputs, inputs], dim=1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        res.append(x)\n",
        "        x = self.block2(x)\n",
        "        res.append(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "\n",
        "        x = self.block8(x)\n",
        "        x = self.block9(x)\n",
        "        x = self.block10(x)\n",
        "        x = self.block11(x)\n",
        "\n",
        "        res.append(x)\n",
        "\n",
        "        x = self.block12(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "\n",
        "        res.append(x)\n",
        "\n",
        "        return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URCtVkjzAC6v"
      },
      "source": [
        "def xception(num_classes, pretrained=True, **kwargs):\n",
        "    \"\"\"Constructs a xception model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    encoder = Xception(num_classes=1000)\n",
        "\n",
        "    if pretrained:\n",
        "        state_dict = model_zoo.load_url(\n",
        "            pretrained_settings['xception']['imagenet']['url'], \n",
        "            model_dir='models'\n",
        "        )\n",
        "        for name, weights in state_dict.items():\n",
        "            if 'pointwise' in name:\n",
        "                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n",
        "        encoder.load_state_dict(state_dict)\n",
        "\n",
        "    model = RetinaNet(encoder=encoder, num_classes=num_classes, **kwargs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcQ917b-y7YS"
      },
      "source": [
        "## Feature Pyramid Network (FPN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucSjdKUfy7l-"
      },
      "source": [
        "class PyramidFeatures(nn.Module):\n",
        "  def __init__(self, C2_size, C3_size, C4_size, C5_size, feature_size=256, use_l2_features=True):\n",
        "    super(PyramidFeatures, self).__init__()\n",
        "    self.use_l2_features = use_l2_features\n",
        "        \n",
        "    # upsample C5 to get P5 from the FPN paper\n",
        "    self.P5_1           = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P5_upsampled   = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    self.P5_2           = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # add P5 elementwise to C4\n",
        "    self.P4_1           = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P4_upsampled   = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    self.P4_2           = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # add P4 elementwise to C3\n",
        "    self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "    self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # add P3 elementwise to C2\n",
        "    self.P2_1 = nn.Conv2d(C2_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
        "    self.P2_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    # \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
        "    self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    # \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
        "    self.P7_1 = nn.ReLU()\n",
        "    self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "\n",
        "    C2, C3, C4, C5 = inputs\n",
        "\n",
        "    P5_x = self.P5_1(C5)\n",
        "    P5_upsampled_x = self.P5_upsampled(P5_x)\n",
        "    P5_x = self.P5_2(P5_x)\n",
        "    \n",
        "    P4_x = self.P4_1(C4)\n",
        "    P4_x = P5_upsampled_x + P4_x\n",
        "    P4_upsampled_x = self.P4_upsampled(P4_x)\n",
        "    P4_x = self.P4_2(P4_x)\n",
        "\n",
        "    P3_x = self.P3_1(C3)\n",
        "    P3_x = P3_x + P4_upsampled_x\n",
        "    P3_upsampled_x = self.P3_upsampled(P3_x)\n",
        "    P3_x = self.P3_2(P3_x)\n",
        "\n",
        "    if self.use_l2_features:\n",
        "      P2_x = self.P2_1(C2)\n",
        "      P2_x = P2_x + P3_upsampled_x\n",
        "      P2_x = self.P2_2(P2_x)\n",
        "\n",
        "      P6_x = self.P6(C5)\n",
        "\n",
        "      P7_x = self.P7_1(P6_x)\n",
        "      P7_x = self.P7_2(P7_x)\n",
        "\n",
        "      if self.use_l2_features:\n",
        "        return [P2_x, P3_x, P4_x, P5_x, P6_x, P7_x]\n",
        "      else:\n",
        "        return [P3_x, P4_x, P5_x, P6_x, P7_x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHhCDcE2AKGy"
      },
      "source": [
        "## Subnets multitask learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVAFtm-YltGH"
      },
      "source": [
        "### Box Regression Subnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsfPxxk9lyCh"
      },
      "source": [
        "class RegressionModel(nn.Module):\n",
        "  \n",
        "  # creates the default regression submodel,\n",
        "  # it predicts regression values for each anchor.\n",
        "  \n",
        "  def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n",
        "    super(RegressionModel, self).__init__()\n",
        "        \n",
        "    self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act3 = nn.ReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act4 = nn.ReLU()\n",
        "\n",
        "    self.output = nn.Conv2d(feature_size, num_anchors*4, kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.act1(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.act2(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.act3(out)\n",
        "\n",
        "    out = self.conv4(out)\n",
        "    out = self.act4(out)\n",
        "\n",
        "    out = self.output(out)\n",
        "\n",
        "    # out is B x C x W x H, with C = 4*num_anchors\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "\n",
        "    return out.contiguous().view(out.shape[0], -1, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOYAab_Dlzcy"
      },
      "source": [
        "### Classification Subnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUqD-bwfl3jV"
      },
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "\n",
        "  # creates the classification submodel, \n",
        "  # it predicts classes for each anchor.\n",
        "\n",
        "  def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256, dropout=0.5):\n",
        "    super(ClassificationModel, self).__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.num_anchors = num_anchors\n",
        "        \n",
        "    self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.ReLU()\n",
        "\n",
        "    self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act3 = nn.ReLU()\n",
        "\n",
        "    self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
        "    self.act4 = nn.ReLU()\n",
        "\n",
        "    self.output = nn.Conv2d(feature_size, num_anchors*num_classes, kernel_size=3, padding=1)\n",
        "    self.output_act = nn.Sigmoid()\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.act1(out)\n",
        "\n",
        "    out = self.conv2(out)\n",
        "    out = self.act2(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    out = self.act3(out)\n",
        "\n",
        "    out = self.conv4(out)\n",
        "    out = self.act4(out)\n",
        "\n",
        "    if self.dropout > 0:\n",
        "      out = F.dropout(out, self.dropout, self.training)\n",
        "\n",
        "    out = self.output(out)\n",
        "    out = self.output_act(out)\n",
        "\n",
        "    # out is B x C x W x H, with C = n_classes + n_anchors\n",
        "    out1 = out.permute(0, 2, 3, 1)\n",
        "\n",
        "    batch_size, width, height, channels = out1.shape\n",
        "\n",
        "    out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n",
        "\n",
        "    return out2.contiguous().view(x.shape[0], -1, self.num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkukUbRRl_tm"
      },
      "source": [
        "### Global Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrWfKgkBmBvF"
      },
      "source": [
        "class GlobalClassificationModel(nn.Module):\n",
        "  def __init__(self, num_features_in, num_classes=80, feature_size=256, dropout=0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, dilation=1, padding=0)\n",
        "    self.fc = nn.Linear(feature_size*2, num_classes)\n",
        "    self.output_act = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(x, 2)\n",
        "    out = self.conv1(out)\n",
        "    out = F.relu(out)\n",
        "\n",
        "    #if self.dropout > 0:\n",
        "    #  out = F.dropout(out, self.dropout, self.training)\n",
        "\n",
        "    avg_pool = F.avg_pool2d(out, out.shape[2:])\n",
        "    max_pool = F.max_pool2d(out, out.shape[2:])\n",
        "    avg_max_pool = torch.cat((avg_pool, max_pool), 1)\n",
        "    out = avg_max_pool.view(avg_max_pool.size(0), -1)\n",
        "\n",
        "    if self.dropout > 0:\n",
        "      out = F.dropout(out, self.dropout, self.training)\n",
        "\n",
        "    out = self.fc(out)\n",
        "    out = self.output_act(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDOl0-3CAewa"
      },
      "source": [
        "## Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZSOUs3-AgYo"
      },
      "source": [
        "def calc_iou(a, b):\n",
        "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
        "\n",
        "    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n",
        "    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n",
        "\n",
        "    iw = torch.clamp(iw, min=0)\n",
        "    ih = torch.clamp(ih, min=0)\n",
        "\n",
        "    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n",
        "\n",
        "    ua = torch.clamp(ua, min=1e-8)\n",
        "\n",
        "    intersection = iw * ih\n",
        "\n",
        "    IoU = intersection / ua\n",
        "\n",
        "    return IoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-xXQHGNAjLL"
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    #def __init__(self):\n",
        "\n",
        "    def forward(self, classifications, regressions, anchors, annotations):\n",
        "        alpha = 0.25\n",
        "        gamma = 2.0\n",
        "        batch_size = classifications.shape[0]\n",
        "        classification_losses = []\n",
        "        regression_losses = []\n",
        "\n",
        "        anchor = anchors[0, :, :]\n",
        "\n",
        "        anchor_widths  = anchor[:, 2] - anchor[:, 0]\n",
        "        anchor_heights = anchor[:, 3] - anchor[:, 1]\n",
        "        anchor_ctr_x   = anchor[:, 0] + 0.5 * anchor_widths\n",
        "        anchor_ctr_y   = anchor[:, 1] + 0.5 * anchor_heights\n",
        "\n",
        "        for j in range(batch_size):\n",
        "\n",
        "            classification = classifications[j, :, :]\n",
        "            regression = regressions[j, :, :]\n",
        "\n",
        "            bbox_annotation = annotations[j, :, :]\n",
        "            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n",
        "\n",
        "            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n",
        "\n",
        "            if bbox_annotation.shape[0] == 0:\n",
        "                # regression_losses.append(torch.tensor(0).float().cuda())\n",
        "                # classification_losses.append(torch.tensor(0).float().cuda())\n",
        "                IoU = torch.zeros(anchor.shape[0], 1).float().cuda()\n",
        "                bbox_annotation = torch.ones(1, 5).float().cuda() * -1\n",
        "                # continue\n",
        "            else:\n",
        "                IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4])  # num_anchors x num_annotations\n",
        "\n",
        "            IoU_max, IoU_argmax = torch.max(IoU, dim=1)  # num_anchors x 1\n",
        "\n",
        "            #import pdb\n",
        "            #pdb.set_trace()\n",
        "\n",
        "            # compute the loss for classification\n",
        "            targets = torch.ones(classification.shape) * -1\n",
        "            targets = targets.cuda()\n",
        "\n",
        "            targets[torch.lt(IoU_max, 0.4), :] = 0\n",
        "\n",
        "            positive_indices = torch.ge(IoU_max, 0.5)\n",
        "\n",
        "            num_positive_anchors = positive_indices.sum()\n",
        "\n",
        "            assigned_annotations = bbox_annotation[IoU_argmax, :]\n",
        "\n",
        "            targets[positive_indices, :] = 0\n",
        "            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1\n",
        "\n",
        "            alpha_factor = torch.ones(targets.shape).cuda() * alpha\n",
        "\n",
        "            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n",
        "            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n",
        "            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n",
        "\n",
        "            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n",
        "\n",
        "            # cls_loss = focal_weight * torch.pow(bce, gamma)\n",
        "            cls_loss = focal_weight * bce\n",
        "\n",
        "            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape).cuda())\n",
        "\n",
        "            classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0))\n",
        "\n",
        "            # compute the loss for regression\n",
        "\n",
        "            if positive_indices.sum() > 0:\n",
        "                assigned_annotations = assigned_annotations[positive_indices, :]\n",
        "\n",
        "                anchor_widths_pi = anchor_widths[positive_indices]\n",
        "                anchor_heights_pi = anchor_heights[positive_indices]\n",
        "                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n",
        "                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n",
        "\n",
        "                gt_widths  = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n",
        "                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n",
        "                gt_ctr_x   = assigned_annotations[:, 0] + 0.5 * gt_widths\n",
        "                gt_ctr_y   = assigned_annotations[:, 1] + 0.5 * gt_heights\n",
        "\n",
        "                # clip widths to 1\n",
        "                gt_widths  = torch.clamp(gt_widths, min=1)\n",
        "                gt_heights = torch.clamp(gt_heights, min=1)\n",
        "\n",
        "                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n",
        "                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n",
        "                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n",
        "                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n",
        "\n",
        "                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))\n",
        "                targets = targets.t()\n",
        "\n",
        "                targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda()\n",
        "\n",
        "\n",
        "                #negative_indices = 1 - positive_indices\n",
        "\n",
        "                regression_diff = torch.abs(targets - regression[positive_indices, :])\n",
        "\n",
        "                regression_loss = torch.where(\n",
        "                    torch.le(regression_diff, 1.0 / 9.0),\n",
        "                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n",
        "                    regression_diff - 0.5 / 9.0\n",
        "                )\n",
        "                regression_losses.append(regression_loss.mean())\n",
        "            else:\n",
        "                regression_losses.append(torch.tensor(0).float().cuda())\n",
        "\n",
        "        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0, keepdim=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCahLVgLYmDm"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiClae2uYmfz"
      },
      "source": [
        "class RetinaNet(nn.Module):\n",
        "  \n",
        "  def __init__(self, encoder: Encoder, num_classes, dropout_cls=0.5,\n",
        "              dropout_global_cls=0.5, use_l2_features=True):\n",
        "    super(RetinaNet, self).__init__()\n",
        "    \n",
        "    fpn_sizes = encoder.fpn_sizes\n",
        "    self.use_l2_features = use_l2_features\n",
        "\n",
        "    self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2], fpn_sizes[3], use_l2_features=use_l2_features)\n",
        "\n",
        "    self.regressionModel = RegressionModel(256)\n",
        "    self.classificationModel = ClassificationModel(256, num_classes=num_classes, dropout=dropout_cls)\n",
        "    self.globalClassificationModel = GlobalClassificationModel(fpn_sizes[-1], num_classes=3, feature_size=256, dropout=dropout_global_cls)\n",
        "    self.globalClassificationLoss = nn.NLLLoss()\n",
        "\n",
        "    if use_l2_features:\n",
        "      pyramid_levels = [2, 3, 4, 5, 6, 7]\n",
        "    else:\n",
        "      pyramid_levels = [3, 4, 5, 6, 7]\n",
        "\n",
        "    self.anchors = Anchors(pyramid_levels=pyramid_levels)\n",
        "\n",
        "    self.regressBoxes = BBoxTransform()\n",
        "\n",
        "    self.clipBoxes = ClipBoxes()\n",
        "\n",
        "    self.focalLoss = FocalLoss()\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "    self.encoder = encoder\n",
        "\n",
        "    prior = 0.01\n",
        "\n",
        "    self.classificationModel.output.weight.data.fill_(0)\n",
        "    self.classificationModel.output.bias.data.fill_(-math.log((1.0 - prior) / prior))\n",
        "\n",
        "    self.regressionModel.output.weight.data.fill_(0)\n",
        "    self.regressionModel.output.bias.data.fill_(0)\n",
        "\n",
        "    self.freeze_bn()\n",
        "    \n",
        "  def freeze_bn(self):\n",
        "    \"\"\"Freeze BatchNorm layers.\"\"\"\n",
        "    for layer in self.modules():\n",
        "      if isinstance(layer, nn.BatchNorm2d):\n",
        "        layer.eval()\n",
        "  \n",
        "  def freeze_encoder(self):\n",
        "    self.encoder.eval()\n",
        "    # correct version, but keep original as model has been trained this way\n",
        "    # for param in self.encoder.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "  def unfreeze_encoder(self):\n",
        "    for param in self.encoder.parameters():\n",
        "      param.requires_grad = True\n",
        "  \n",
        "  def boxes(self, img_batch, regression, classification, global_classification, anchors):\n",
        "    transformed_anchors = self.regressBoxes(anchors, regression)\n",
        "    transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n",
        "\n",
        "    scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
        "\n",
        "    scores_over_thresh = (scores > 0.025)[0, :, 0]\n",
        "\n",
        "    if scores_over_thresh.sum() == 0:\n",
        "      # no boxes to NMS, just return\n",
        "      return [torch.zeros(0), global_classification, torch.zeros(0, 4)]\n",
        "    else:\n",
        "      classification = classification[:, scores_over_thresh, :]\n",
        "      transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
        "      scores = scores[:, scores_over_thresh, :]\n",
        "\n",
        "      # use very low threshold of 0.05 as boxes should not overlap\n",
        "      anchors_nms_idx = nms(torch.cat([transformed_anchors, scores], dim=2)[0, :, :], 0.05)\n",
        "\n",
        "      nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n",
        "      return [nms_scores, global_classification, transformed_anchors[0, anchors_nms_idx, :]]\n",
        "\n",
        "  def forward(self, inputs, return_loss, return_boxes, return_raw=False):\n",
        "    \n",
        "    if return_loss:\n",
        "      img_batch, annotations, global_annotations = inputs\n",
        "    else:\n",
        "      img_batch = inputs\n",
        "\n",
        "    x1, x2, x3, x4 = self.encoder.forward(img_batch)\n",
        "\n",
        "    features = self.fpn([x1, x2, x3, x4])\n",
        "\n",
        "    regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n",
        "\n",
        "    classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n",
        "\n",
        "    global_classification = self.globalClassificationModel(x4)\n",
        "    \n",
        "    anchors = self.anchors(img_batch)\n",
        "\n",
        "    if return_raw:\n",
        "      return [regression, classification, torch.exp(global_classification), anchors]\n",
        "\n",
        "    res = []\n",
        "\n",
        "    if return_loss:\n",
        "      res += list(self.focalLoss(classification, regression, anchors, annotations))\n",
        "      res += [self.globalClassificationLoss(global_classification, global_annotations)]\n",
        "\n",
        "    if return_boxes:\n",
        "      res += self.boxes(img_batch=img_batch,\n",
        "                        regression=regression,\n",
        "                        classification=classification,\n",
        "                        global_classification=global_classification,\n",
        "                        anchors=anchors)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPsPRD50X1ZZ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfgYa5YiDFEr"
      },
      "source": [
        "\"\"\"\n",
        "Validate model at the epoch end \n",
        "    \n",
        "Args: \n",
        "    retinanet: current model \n",
        "    dataloader_valid: dataloader for the validation fold\n",
        "    epoch_num: current epoch\n",
        "    save_oof: boolean flag, if calculate oof predictions and save them in pickle (oof = Out-of-Fold)\n",
        "    predictions_dir: directory for saving predictions\n",
        "\n",
        "Outputs:\n",
        "    loss_hist_valid: total validation loss, history \n",
        "    loss_cls_hist_valid, loss_cls_global_hist_valid: classification validation losses\n",
        "    loss_reg_hist_valid: regression validation loss\n",
        "\"\"\"\n",
        "\n",
        "def validation(\n",
        "    retinanet: nn.Module, \n",
        "    val_dataloader: nn.Module, \n",
        "    epoch_num: int, \n",
        "    predictions_dir: str, \n",
        "    save_oof=True,\n",
        ") -> tuple:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        #Â Set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "        retinanet.eval()\n",
        "\n",
        "        loss_hist_valid, loss_cls_hist_valid, loss_cls_global_hist_valid, loss_reg_hist_valid = [],[],[],[]\n",
        "        \n",
        "        # Show a smart progress bar\n",
        "        data_iter = tqdm(enumerate(val_dataloader), total=len(val_dataloader))\n",
        "        \n",
        "        \n",
        "        if save_oof:\n",
        "            oof = collections.defaultdict(list)\n",
        "        \n",
        "        for iter_num, data in data_iter:\n",
        "            # Run model and save the return values of the model (see Model section for further info)\n",
        "            (\n",
        "                classification_loss,\n",
        "                regression_loss,\n",
        "                global_classification_loss,\n",
        "                nms_scores,\n",
        "                global_class,\n",
        "                transformed_anchors,\n",
        "            ) = retinanet(\n",
        "                [\n",
        "                  data['img'].cuda().float(),       \n",
        "                  data['annot'].cuda().float(),          \n",
        "                  data['category'].cuda()\n",
        "                ],\n",
        "                return_loss=True,\n",
        "                return_boxes=True,\n",
        "            )\n",
        "\n",
        "            # Save out of fold predictions\n",
        "            if save_oof:\n",
        "                # predictions\n",
        "                oof[\"gt_boxes\"].append(data[\"annot\"].cpu().numpy().copy())\n",
        "                oof[\"gt_category\"].append(data[\"category\"].cpu().numpy().copy())\n",
        "                oof[\"boxes\"].append(transformed_anchors.cpu().numpy().copy())\n",
        "                oof[\"scores\"].append(nms_scores.cpu().numpy().copy())\n",
        "                oof[\"category\"].append(global_class.cpu().numpy().copy())\n",
        "\n",
        "            # Compute losses\n",
        "            classification_loss = classification_loss.mean()\n",
        "            regression_loss = regression_loss.mean()\n",
        "            global_classification_loss = global_classification_loss.mean()\n",
        "            loss = classification_loss + regression_loss + global_classification_loss * 0.1\n",
        "\n",
        "            # Add the current losses result to the loss history (list)\n",
        "            loss_hist_valid.append(float(loss))\n",
        "            loss_cls_hist_valid.append(float(classification_loss))\n",
        "            loss_cls_global_hist_valid.append(float(global_classification_loss))\n",
        "            loss_reg_hist_valid.append(float(regression_loss))\n",
        "            data_iter.set_description(\n",
        "                f\"{epoch_num} cls: {np.mean(loss_cls_hist_valid):1.4f} cls g: {np.mean(loss_cls_global_hist_valid):1.4f} Reg: {np.mean(loss_reg_hist_valid):1.4f} Loss {np.mean(loss_hist_valid):1.4f}\"\n",
        "            )\n",
        "            del classification_loss\n",
        "            del regression_loss\n",
        "\n",
        "        ##TO DO\n",
        "        ##if save_oof:  # save predictions\n",
        "        ##    pickle.dump(oof, open(f\"{predictions_dir}/{epoch_num:03}.pkl\", \"wb\"))\n",
        "\n",
        "    return loss_hist_valid, loss_cls_hist_valid, loss_cls_global_hist_valid, loss_reg_hist_valid\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfJ9pt69Ui5y"
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import gc \n",
        "\n",
        "def training_f(\n",
        "    model_name: str,\n",
        "    epochs: int,\n",
        "    resume_weights: str=\"\",\n",
        "    resume_epoch: int=0):\n",
        "\n",
        "  pretrained = True\n",
        "\n",
        "  if model_name == 'resnet50':\n",
        "    retinanet = resnet50(1, pretrained)\n",
        "  elif model_name == 'se_resnext50':\n",
        "    retinanet = se_resnext50(1, pretrained)\n",
        "  elif model_name == 'pnasnet5':\n",
        "    retinanet = pnasnet5(1, pretrained)\n",
        "  elif model_name == 'xception':\n",
        "    retinanet = xception(1, pretrained)\n",
        "\n",
        "  # TODO crea cartelle checkpoints\n",
        "  checkpoints_dir = '/content/drive/My Drive/Neural/checkpoints'\n",
        "  predictions_dir = '/content/drive/My Drive/Neural/predictions'\n",
        "  os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "  os.makedirs(predictions_dir, exist_ok=True)\n",
        "  \n",
        "  # load weights to continue training\n",
        "  if resume_weights != \"\":\n",
        "    print(\"load model from: \", resume_weights)\n",
        "    retinanet = torch.load(resume_weights).cuda()\n",
        "  else:\n",
        "    retinanet = retinanet.to(device)\n",
        "  \n",
        "  retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
        "\n",
        "  retinanet.training = True\n",
        "  optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
        "  \n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, patience=4, verbose=True, factor=0.2\n",
        "  )\n",
        "  scheduler_by_epoch = False\n",
        "  \n",
        "  # ADDED FROM OTHER FILES\n",
        "  loss_hist = []\n",
        "\n",
        "  #for epoch_num in range(resume_epoch+1, epochs):\n",
        "  for epoch_num in range(epochs):  \n",
        "    retinanet.train() \n",
        "    \n",
        "    if epoch_num < 1:\n",
        "      # train FC layers with freezed encoder for the first epoch\n",
        "      retinanet.module.freeze_encoder()  \n",
        "    else:\n",
        "      retinanet.module.unfreeze_encoder()\n",
        "    \n",
        "    retinanet.module.freeze_bn()\n",
        "\n",
        "    # losses\n",
        "    epoch_loss, loss_cls_hist, loss_cls_global_hist, loss_reg_hist = [], [], [], []\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "      data_iter = tqdm(enumerate(train_dataloader), total = len(train_dataloader))\n",
        "\n",
        "      for iter_num, data in data_iter:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = [\n",
        "                  data['img'].cuda().float(),      #image\n",
        "                  data['annot'].cuda().float(),    #boxes\n",
        "                  data['category'].cuda()\n",
        "        ]\n",
        "\n",
        "        (classification_loss, regression_loss, global_classification_loss,) = retinanet(\n",
        "            inputs, return_loss=True, return_boxes=False\n",
        "            )\n",
        "\n",
        "        classification_loss = classification_loss.mean() \n",
        "        regression_loss = regression_loss.mean()\n",
        "        global_classification_loss = global_classification_loss.mean()\n",
        "        loss = classification_loss + regression_loss + global_classification_loss*0.1\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.05)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # loss history\n",
        "        loss_cls_hist.append(float(classification_loss))\n",
        "        loss_cls_global_hist.append(float(global_classification_loss))\n",
        "        loss_reg_hist.append(float(regression_loss))\n",
        "\n",
        "        loss_hist.append(float(loss)) #preso da altro file\n",
        "        epoch_loss.append(float(loss))\n",
        "        \n",
        "        print(\n",
        "          'Epoch: {} | Iteration: {} | \\n\\t\\tClassification loss: {:1.5f} | Regression loss: {:1.5f} | \\n\\t\\tGlobal loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
        "            epoch_num, iter_num, float(classification_loss), float(regression_loss), float(global_classification_loss), np.mean(loss_hist))\n",
        "        )\n",
        "\n",
        "        # print losses with tqdm interator\n",
        "        #data_iter.set_description(\n",
        "        #  f\"{epoch_num} cls: {np.mean(loss_cls_hist):1.4f} cls g: {np.mean(loss_cls_global_hist):1.4f} Reg: {np.mean(loss_reg_hist):1.4f} Loss: {np.mean(epoch_loss):1.4f}\"\n",
        "        #)\n",
        "        del classification_loss\n",
        "        del regression_loss\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # TODO save model checkpoints \n",
        "    #torch.save(retinanet.module, f\"{checkpoints_dir}/{model_name}_{epoch_num:03}.pt\")\n",
        "\n",
        "    # validation\n",
        "    (\n",
        "      loss_hist_valid,\n",
        "      loss_cls_hist_valid,\n",
        "      loss_cls_global_hist_valid,\n",
        "      loss_reg_hist_valid,\n",
        "    ) = validation(retinanet,\n",
        "        val_dataloader,\n",
        "        epoch_num,\n",
        "        predictions_dir,\n",
        "        save_oof=True,\n",
        "    )\n",
        "  \n",
        "    # log validation loss history\n",
        "    #logger.scalar_summary(\"loss_valid\", np.mean(loss_hist_valid), epoch_num)\n",
        "    #logger.scalar_summary(\"loss_valid_classification\", np.mean(loss_cls_hist_valid), epoch_num)\n",
        "    #logger.scalar_summary(\n",
        "    #  \"loss_valid_global_classification\", np.mean(loss_cls_global_hist_valid), epoch_num,\n",
        "    #)\n",
        "    #logger.scalar_summary(\"loss_valid_regression\", np.mean(loss_reg_hist_valid), epoch_num)\n",
        "  \n",
        "    #scheduler.step(np.mean(loss_reg_hist_valid))\n",
        "  \n",
        "\n",
        "  retinanet.eval()\n",
        "  torch.save(retinanet, f\"{checkpoints_dir}/{model_name}_final.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481,
          "referenced_widgets": [
            "b7ddd8ece0b245fab954d77f24292604",
            "b755b4ebdd674c56afb047fc5bc3d581",
            "4e80f9d90fd2437f9c718b5fd023905e",
            "19af2fc9c1164960bf5a7bb130ef37ba",
            "cb06a7325a674c8b89e3222413832e54",
            "4a9e3dc2d3ea46ebbbf5c3ca32e3fe9a",
            "1ca03cf6e40941699e7e331375985ae6",
            "a86938d765a1409a875859a146d236ae"
          ]
        },
        "id": "9OV3AenCdhXP",
        "outputId": "e7157dfe-eda6-460b-bfe5-915b1286f7ed"
      },
      "source": [
        "## RUNNER ##\n",
        "\n",
        "model_name = \"resnet50\"\n",
        "#model_name = \"se_resnext50\"\n",
        "#model_name = \"pnasnet5\"\n",
        "#model_name = \"xception\"\n",
        "\n",
        "print(\"TRAINING STARTED\")\n",
        "\n",
        "training_f(model_name, 5, '', 0)\n",
        "\n",
        "print(\"TRAINING FINISHED\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING STARTED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7ddd8ece0b245fab954d77f24292604",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6021 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-4c81d52baeff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAINING STARTED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtraining_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAINING FINISHED\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-3c775d982078>\u001b[0m in \u001b[0;36mtraining_f\u001b[0;34m(model_name, epochs, resume_weights, resume_epoch)\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-5df857142763>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.dcm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNplzd5l5O6u"
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYuiyl5UywaV"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBIYE-9MyxnF"
      },
      "source": [
        "\"\"\"\n",
        "Loads model weights from the checkpoint, plots ground truth and predictions\n",
        "\n",
        "Args: \n",
        "    model_name : string name from the models configs listed in models.py file\n",
        "    fold       : evaluation fold number, 0-3\n",
        "    debug      : if True, runs debugging on few images \n",
        "    checkpoint : directory with weights (if avaialable) \n",
        "    pics_dir   : directory for saving prediction images \n",
        "    \n",
        "\"\"\"\n",
        "def test(\n",
        "    model_name: str,\n",
        "    img_size: int, \n",
        "    fold: int, \n",
        "    debug: bool, \n",
        "    checkpoint: str, \n",
        "    pics_dir: str\n",
        "    ):\n",
        "     \n",
        "    # Load model\n",
        "    model = torch.load(checkpoint)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Show a smart progress bar\n",
        "    data_iter = tqdm(enumerate(test_dataloader), total=len(test_dataloader))\n",
        "\n",
        "    for iter_num, data in data_iter:\n",
        "\n",
        "        # Run model and save the return values of the model (see Model section for further info)\n",
        "        (\n",
        "            classification_loss,\n",
        "            regression_loss,\n",
        "            global_classification_loss,\n",
        "            nms_scores,\n",
        "            nms_class,\n",
        "            transformed_anchors,\n",
        "        ) = model(\n",
        "            [\n",
        "               data['img'].cuda().float(),       \n",
        "               data['annot'].cuda().float(),          \n",
        "               data['category'].cuda()\n",
        "            ],\n",
        "            return_loss=True,\n",
        "            return_boxes=True,\n",
        "        )\n",
        "\n",
        "        nms_scores = nms_scores.cpu().detach().numpy()\n",
        "        nms_class = nms_class.cpu().detach().numpy()\n",
        "        transformed_anchors = transformed_anchors.cpu().detach().numpy()\n",
        "\n",
        "        # Print results\n",
        "        print(f\"nms_scores {nms_scores}, transformed_anchors.shape {transformed_anchors.shape}\")\n",
        "        print(f\"cls loss: {float(classification_loss)}, global cls loss: {global_classification_loss}, reg loss: {float(regression_loss)}\")\n",
        "        print(f\"category: {data[\"category\"].numpy()[0]} {np.exp(nms_class[0])} {dataset_valid.categories[data[\"category\"][0]]}\")\n",
        "\n",
        "        # PLOT RESULTS\n",
        "\n",
        "        # plot data and ground truth\n",
        "        plt.figure(iter_num, figsize=(6, 6))\n",
        "        plt.cla()\n",
        "        plt.imshow(data[\"img\"][0, 0].cpu().detach().numpy(), cmap=plt.cm.gist_gray)\n",
        "        plt.axis(\"off\")\n",
        "        gt = data[\"annot\"].cpu().detach().numpy()[0]\n",
        "        for i in range(gt.shape[0]):\n",
        "            if np.all(np.isfinite(gt[i])):\n",
        "                p0 = gt[i, 0:2]\n",
        "                p1 = gt[i, 2:4]\n",
        "                plt.gca().add_patch(\n",
        "                    plt.Rectangle(\n",
        "                        p0,\n",
        "                        width=(p1 - p0)[0],\n",
        "                        height=(p1 - p0)[1],\n",
        "                        fill=False,\n",
        "                        edgecolor=\"b\",\n",
        "                        linewidth=2,\n",
        "                    )\n",
        "                )\n",
        "        \n",
        "        # add predicted boxes to the plot\n",
        "        for i in range(len(nms_scores)):\n",
        "            nms_score = nms_scores[i]\n",
        "            if nms_score < 0.1:\n",
        "                break\n",
        "            p0 = transformed_anchors[i, 0:2]\n",
        "            p1 = transformed_anchors[i, 2:4]\n",
        "            color = \"r\"\n",
        "            if nms_score < 0.3:\n",
        "                color = \"y\"\n",
        "            if nms_score < 0.25:\n",
        "                color = \"g\"\n",
        "            plt.gca().add_patch(\n",
        "                plt.Rectangle(\n",
        "                    p0,\n",
        "                    width=(p1 - p0)[0],\n",
        "                    height=(p1 - p0)[1],\n",
        "                    fill=False,\n",
        "                    edgecolor=color,\n",
        "                    linewidth=2,\n",
        "                )\n",
        "            )\n",
        "            plt.gca().text(p0[0], p0[1], f\"{nms_score:.3f}\", color=color)\n",
        "        plt.show()\n",
        "\n",
        "        os.makedirs(pics_dir, exist_ok=True)\n",
        "        plt.savefig(\n",
        "            f\"{pics_dir}/predict_{iter_num}.eps\", dpi=300, bbox_inches=\"tight\", pad_inches=0,\n",
        "        )\n",
        "        plt.savefig(\n",
        "            f\"{pics_dir}/predict_{iter_num}.png\", dpi=300, bbox_inches=\"tight\", pad_inches=0,\n",
        "        )\n",
        "        plt.close()\n",
        "\n",
        "        print(nms_scores)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}